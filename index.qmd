---
title: "Parallel Worlds"
author: "Francesco Prem Solidoro[,](https://www.youtube.com/watch?v=dQw4w9WgXcQ&pp=ygUYbmV2ZXIgZ29ubmEgZ2l2ZSB5b3UgdXAg) Michele Salvi"
format:
  typst:
    toc: true
    section-numbering: 1.1.a
    columns: 1
    bibliography: references.yml
    bibliographystyle: apa
execute:
  warning: false
  cache: true
  echo: false
editor: visual
---

# Parallel Worlds - a View of Social Media Manipulation and Polarization

-   Michele Salvi / michele.salvi4\@studio.unibo.it / 0001082457
-   Francesco Prem Solidoro / francesco.solidoro\@studio.unibo.it / 0001071898

## Abstract

What is truth? It can be argued that truth is but the set of facts we intersubjectively agree to be correct as a society. In this view, while voices of specialists hold power sway, it's ultimately public opinion that shapes the shared ground over which political discourse and policy are built. It immediately becomes apparent that choosing one's terrain is a powerful tool in achieving one's objectives. To that end, it becomes essential to: - win over people to believe in your cause - make sure people you won over don't leave for other causes In comes "Parallel Worlds". There's no stronger attack against opposing ideas than die-hard fans, and no defense more solid than said die-hard fans *never beign exposed to competing ideas*. The pursuit for achieving a parallel world with a Truth that's centralised is an old one: think of Mussolini and Hitler's use of radio. The renewed threat coming from it is that we're coming off of a new media revolution: just like the printing press, the radio and the television before it, the internet has radically changed the way information flows through society. Analogously, with this revolution, new paradigms of communiction have burst into the scene, alongside methods for manipulation of information. The Internet has the unique property of allowign anyone to have a voice, and therefore to represent in an uncanny way the dynamics of social converstation, where not only the loudest, but also the most numerous voices dominate. In this there's an inherit democratic principle of attention being divided equally through society. In this there's also an inherent danger: with bots and technologicla advancment, it's become easy to artificially boost ideas to favour an agenda through a faux legitimisation from a virtual majority. With the objective of enclosing people who believe they're engaged in an open network into a closed down bubble of ideas, thus spawns the Astroturf. A successful astroturf will appear to be grassroots support for an idea, while driving polarisation to a point where those in support and those in opposition to the target idea will effectively live in different, parallel worlds. With all contact being auspicably hostile: demonisation of the other side, deification of one's one, discreditation of third party "experts". These are all hallmarks of astroturf campaigns, and they're more and more visible within modern society, as this camflauged threat rips the promise of the open internet into bubbles floating in the sea of its former potential.

## Introduction

The topic of manipulation has long been studied by scholars in its different facets, and in this paper we'll explore the connection between the political sphere and manipulation when used as an instrument to achieve control.

In the last years social media usage has skyrocketed @rise, thus more advanced methods of manipulating public opinion, both in and outside the political sphere, have arisen due to the innovations and reach that social media platforms boast. Historically, the preferred outcome of manipulation has always been polarization, giving rise to strong leaders and divides @pol-hist.

The Merriam-Webster dictionary defines polarization @pol-def as "a state in which the opinions, beliefs, or interests of a group or society no longer range along a continuum but become concentrated at opposing extremes", it's a key factor to view polarization as an outcome rather than a means, since it's always been the objective of different political parties to achieve their goals. Now, in this era of limitless communication, one would see complete polarization as harder to achieve, due to the sheer amount of different opinions available on social media, but there's a new, and arguably the worst ever, means of achieving a polarized society: astroturfing.

Astroturfing is defined by the Merriam-Webster dictionary @astro-def as "organized activity that is intended to create a false impression of a widespread, spontaneously arising, grassroots movement in support of or in opposition to something but that is in reality initiated and controlled by a concealed group or organization". Such a practice is deeply deceptive and thrives on platforms like social media, where identity can be disguised and allegiance faked, to distort and manipulate public opinion.

Astroturfing can be declined into two further classifications, *commercial astroturfing*, centered around the accumulation of profit, and *political astroturfing*, concerned with the attainment of political objectives. Research on political astroturfing has become increasingly present in the last years, and it mainly focuses on analyzing its effects on politics, highlighting the power that this practice represents by exposing its presence in (but not limited to) South Korea's 2012 Presidential Elections @astro-sudkr, the USA's 2016 elections @astro-usa2016 and faking Chinese Social media posts @astro-china. These papers highlight the real-world effects and the seriousness of groups and/or individuals rich in economic and social capital, who seek to manipulate public opinion to achieve private objectives, especially in politics, putting them in commanding positions.

The idea behind the "parallel worlds" is that polarization through astroturfing acts silently and unbeknownst to the population, creating fissures and fragmentation in public opinion that effectively alienate and polarize society. The preferred medium for this practice is, as previously stated, through social media, so we focused on the biggest and most documented instance of astroturfing, the 2016 USA elections.

## Research Question

In our pursuit of understanding how parallel worlds are built, we have to understand two things:

-   what makes an astroturf?

-   what effect does it actually have?

-   is it possible to generalize and model an astroturfing from the words used?

    To understand this, we will be conducting a sentiment analysis over the datasets of IRA-affiliated tweets utilizing text mining practices such as word frequencies, tf-idf, sentiment analysis through the NRC, Bing and AFINN lexicons, and the aid of LDA, latent dirichlet allocation, to model the topics discussed in the tweets.

```{r}
library(tidyverse)
library(paletteer)
library(ggthemes)
library(knitr)
library(psych)
library(readr)
library(ggpmisc)
library(tidytext)
library(tm)
library(gridExtra)
library(topicmodels)
library(ldatuning)
library(caret)
library(textclean)
library(igraph)
library(ggraph)
```

### Downloading, Importing, merging and cleaning the dataset

We got the following datasets from [538's github](github.com/fivethirdyeight/russian-troll-tweets), which were merged into a single dataset.

```{r}
IRAtweets1 <- read_csv("data/russian-troll-tweets/IRAhandle_tweets_1.csv")
IRAtweets2 <- read_csv("data/russian-troll-tweets/IRAhandle_tweets_2.csv")
IRAtweets3 <- read_csv("data/russian-troll-tweets/IRAhandle_tweets_3.csv")
IRAtweets4 <- read_csv("data/russian-troll-tweets/IRAhandle_tweets_4.csv")
IRAtweets5 <- read_csv("data/russian-troll-tweets/IRAhandle_tweets_5.csv")
IRAtweets6 <- read_csv("data/russian-troll-tweets/IRAhandle_tweets_6.csv")
IRAtweets7 <- read_csv("data/russian-troll-tweets/IRAhandle_tweets_7.csv")
IRAtweets8 <- read_csv("data/russian-troll-tweets/IRAhandle_tweets_8.csv")
IRAtweets9 <- read_csv("data/russian-troll-tweets/IRAhandle_tweets_9.csv")
IRAtweets10 <- read_csv("data/russian-troll-tweets/IRAhandle_tweets_10.csv")
IRAtweets11 <- read_csv("data/russian-troll-tweets/IRAhandle_tweets_11.csv")
IRAtweets12 <- read_csv("data/russian-troll-tweets/IRAhandle_tweets_12.csv")
IRAtweets13 <- read_csv("data/russian-troll-tweets/IRAhandle_tweets_13.csv")

df_list <- list(
              IRAtweets1,
              IRAtweets2,
              IRAtweets3,
              IRAtweets4,
              IRAtweets5,
              IRAtweets6,
              IRAtweets7,
              IRAtweets8,
              IRAtweets9,
              IRAtweets10,
              IRAtweets11,
              IRAtweets12,
              IRAtweets13
)

IRAtweets <- do.call("rbind", df_list)
IRAtweets <- subset(IRAtweets, select=c(-external_author_id,-harvested_date,-new_june_2018,-alt_external_id,-tweet_id,-article_url,-tco1_step1,-tco2_step1,-tco3_step1))


```

Since the analysis is quite taxing, we're going to sample 100.000 tweets to perform a preliminary analysis on. Remove the following block in order to conduct an analysis of the full dataset.

```{r}
set.seed(777)
IRAtweets <- IRAtweets[sample(nrow(IRAtweets), 100000),] #remove line for complete analysis 
```

The cleaned and merged dataset contains the following variables:

-   **author**: the author of the tweet, categorical
-   **content**: the content (text) of the tweet, string
-   **region**: the region in which the tweet "originated", if not specified it's marked as "Unknown", categorical
-   **language**: the language in which the tweet was written , categorical
-   **publish_date**: the date in which the tweet was published, date
-   **following**: the number of accounts followed by the tweet author, numerical
-   **followers**: the number of accounts that follow the tweet author, numerical
-   **updates**: the number of impressions (likes, comments and retweets) summed, numerical
-   **post_type**: the type of post (retweet / quote retweet / NA for normal tweets), categorical
-   **account_type**: the type of account, previously set, by tweet content, categorical
-   **retweet:** weather the tweet was a retweet or not, binary
-   **account_category:** the accounts grouped by categories based on tweet content, categorical

```{r}
#descriptive stuff

mean_imp <- IRAtweets %>%
  select(c(author,updates,region,language)) %>% 
  group_by(author) %>%
  mutate(mean_Impressions = round(mean(updates),digits = 3)) %>%
  arrange(desc(mean_Impressions))

Lang_P <- IRAtweets %>% 
  group_by(language) %>%
  summarize(count = n())%>% 
  arrange(desc(count))%>%
  slice(1:6)%>%
  ungroup() %>%
  ggplot(aes(reorder(language, count),count/10000))+
  geom_bar(stat = "identity",fill = "darkblue")+
  coord_flip()+
  theme_bw()+
  labs(x = "Language", y = "Tweets (tens of thousands)", title = "Most Significant Languages")

Region_P <-  IRAtweets %>% 
  group_by(region) %>%
  summarize(count = n())%>% 
  arrange(desc(count))%>%
  slice(1:6)%>%
  ungroup() %>%
  ggplot(aes(reorder(region, count),count/10000))+
  geom_bar(stat = "identity", fill = "darkred")+
  coord_flip()+
  theme_bw()+
  labs(x = "Region", y = "Tweets (tens of thousands)", title = "Most Significant Regions")

Acctype_P <-  IRAtweets %>% 
  group_by(account_category) %>%
  summarize(count = n())%>% 
  arrange(desc(count))%>%
  ungroup() %>%
  ggplot(aes(reorder(account_category, count),count/10000))+
  geom_bar(stat = "identity", fill = "darkgreen")+
  coord_flip()+
  theme_bw()+
  labs(x = "Account Categories", y = "Tweets (tens of thousands)", title = "Tweet by Account types")

```

To describe the dataset, we plotted a few graphs (some of which are attached, but not essential), showing the proportions of tweet by languages, by account types and by regions (not included in the pdf, code block above)

To represent tweets in proportion to their engagement, so effect on the population, we then plotted tweets by average impressions, according to the regions, hilighting the discrepancies, and different reception of the astroturfing in different regions.

```{r}
totimp_mean <- mean(mean_imp$mean_Impressions)

Meanimp_P <- mean_imp %>%
  group_by(region)%>%
  add_count(region)%>%
  ungroup() %>%
  mutate(rank = dense_rank(desc(n)))%>%
  filter(rank %in% 1:6)%>%
  ggplot(aes(mean_Impressions))+
  geom_histogram(aes(fill = region))+
  geom_vline(aes(xintercept=totimp_mean))+
  scale_fill_paletteer_d("nbapalettes::bucks_city2")+
  theme_bw()+
  labs(x = "Average impressions",y = "", title = "Average Tweet Impressions in significant regions")

Meanimp_P
```

As it can be observed in the graph, despite there being IRA-connected accounts in multiple regions, most of the interactions and impressions come from tweets from the united states.

To represent well the most significant regions (United states, Unknown) we categorized the IRA-connected accounts by their type, so the macro-topic they covered in their tweets, along with the retweet rate and descriptive statistics on the follows and followers.

```{r}
describe(IRAtweets$followers)
describe(IRAtweets$following)

mean(IRAtweets$retweet)

IRAtweets %>%
  group_by(account_category) %>%
  summarize(retweet_rate = mean(retweet)) %>%
  arrange(desc(retweet_rate))

Regionacctype_P <- IRAtweets %>%
  group_by(region)%>%
  add_count(region)%>%
  ungroup() %>%
  mutate(rank = dense_rank(desc(n)))%>%
  filter(rank %in% 1:2)%>%
  distinct(author, .keep_all = T)%>%
  group_by(account_category) %>%
  ggplot(aes(account_category))+
  geom_bar(fill = "#CE1141FF")+
  facet_wrap(vars(region))+
  scale_fill_paletteer_d("nbapalettes::bulls")+
  theme_bw()+
  theme(axis.text.x = element_text(angle = 90))+
  labs(x = "Account Categories",y = "Accounts", title = "Account categories in significant regions")

Regionacctype_P
```

Most of the accounts in the tweets can be traced back to either the US or the "Unknown" region, but since most accounts in this last region are "NonEnglish", we'll discard those and focus on those composed in English for most of our analysis.

Additionally, since the times at which the tweets were posted are included in the dataset, we plot them, progressively restricting them on the date of the presidential election, to have a look at the patterns.

```{r}

Timeseries_P <- IRAtweets %>% 
  mutate(timedates = as.Date(lubridate::parse_date_time(publish_date,"%m/%d/%Y %h:%M")))%>%
  group_by(timedates) %>%
  add_count(timedates)%>%
  filter(timedates > "2015-01-01" & timedates < "2018-01-01") %>% #basically no tweets before
  ggplot( aes(timedates,n))+
  geom_line(color = "#DB3EB1FF")+
  scale_x_date(date_labels = "%Y-%m-%d", date_breaks ="1 year" )+
  stat_peaks(geom = "point", span = 45, color = "#41B6E6FF", size = 2,ignore_threshold = 0.69) +
  stat_peaks(geom = "label", span = 45, color = "#41B6E6FF", angle = 0,
             hjust = -0.1, x.label.fmt = "%Y-%m-%d", ignore_threshold = 0.69)+
  labs(x = "Time",y = "Tweets", title = "Tweets over time")+
  theme_bw()


tweets_1y <- IRAtweets %>% #focusing on election year
  mutate(timedates = as.Date(lubridate::parse_date_time(publish_date,"%m/%d/%Y %h:%M")))%>%
  group_by(timedates) %>%
  add_count(timedates)%>%
  filter(timedates > "2016-01-01" & timedates < "2017-01-01") %>% 
  ggplot( aes(timedates,n))+
  geom_line(color = "#DB3EB1FF")+
  scale_x_date(date_labels = "%Y-%m-%d", date_breaks ="1 year" )+
  stat_peaks(geom = "point", span = 45, color = "#41B6E6FF", size = 2,ignore_threshold = 0.69) +
  stat_peaks(geom = "label", span = 45, color = "#41B6E6FF", angle = 0,
             hjust = -0.1, x.label.fmt = "%Y-%m-%d", ignore_threshold = 0.69)+
  labs(x = "Time",y = "Tweets", title = "Tweets over time")+
  theme_bw()+
  geom_smooth(method = "lm", color = "#1BB6AFFF")

tweets_1y <- IRAtweets %>% #focusing on election year
  mutate(timedates = as.Date(lubridate::parse_date_time(publish_date,"%m/%d/%Y %h:%M")))%>%
  group_by(timedates) %>%
  add_count(timedates)%>%
  filter(timedates > "2016-01-01" & timedates < "2017-01-01") %>% 
  ggplot( aes(timedates,n))+
  geom_line(color = "#DB3EB1FF")+
  scale_x_date(date_labels = "%Y-%m-%d", date_breaks ="1 year" )+
  stat_peaks(geom = "point", span = 45, color = "#41B6E6FF", size = 2,ignore_threshold = 0.69) +
  stat_peaks(geom = "label", span = 45, color = "#41B6E6FF", angle = 0,
             hjust = -0.1, x.label.fmt = "%Y-%m-%d", ignore_threshold = 0.69)+
  labs(x = "Time",y = "Tweets", title = "Tweets 2016-2017")+
  theme_bw()+
  geom_smooth(method = "lm", color = "#1BB6AFFF",formula = 'y ~ x')

tweets_before <- IRAtweets %>% 
  mutate(timedates = as.Date(lubridate::parse_date_time(publish_date,"%m/%d/%Y %h:%M")))%>%
  group_by(timedates) %>%
  add_count(timedates)%>%
  filter(timedates > "2016-08-01" & timedates < "2016-11-10") %>% 
  ggplot( aes(timedates,n))+
  geom_line(color = "#DB3EB1FF")+
  scale_x_date(date_labels = "%Y-%m-%d", date_breaks ="1 year" )+
  stat_peaks(geom = "point", span = 45, color = "#41B6E6FF", size = 2,ignore_threshold = 0.69) +
  stat_peaks(geom = "label", span = 45, color = "#41B6E6FF", angle = 0,
             hjust = -0.1, x.label.fmt = "%Y-%m-%d", ignore_threshold = 0.69)+
  labs(x = "Time",y = "Tweets", title = "Tweets before elections")+
  theme_bw()+
  geom_smooth(method = "lm", color = "#1BB6AFFF",formula = 'y ~ x')

tweets_after <- IRAtweets %>% #focusing on election year
  mutate(timedates = as.Date(lubridate::parse_date_time(publish_date,"%m/%d/%Y %h:%M")))%>%
  group_by(timedates) %>%
  add_count(timedates)%>%
  filter(timedates > "2016-10-01" & timedates < "2016-12-01") %>% 
  ggplot( aes(timedates,n))+
  geom_line(color = "#DB3EB1FF")+
  scale_x_date(date_labels = "%Y-%m-%d", date_breaks ="1 year" )+
  stat_peaks(geom = "point", span = 45, color = "#41B6E6FF", size = 2,ignore_threshold = 0.69) +
  stat_peaks(geom = "label", span = 45, color = "#41B6E6FF", angle = 0,
             hjust = -0.1, x.label.fmt = "%Y-%m-%d", ignore_threshold = 0.69)+
  labs(x = "Time",y = "Tweets", title = "Tweets right after elections")+
  theme_bw()+
  geom_smooth(method = "lm", color = "#1BB6AFFF", formula = 'y ~ x' )

grid.arrange(Timeseries_P,tweets_1y, tweets_before, tweets_after,nrow = 2 )
```

As it can be observed, tweets steadily increased until a few days after the election, when they started dropping.

## Analysis

### Text analysis

We began our analysis by focusing on the tweet's text. In this type of "word" centered approach we filtered the tweets to contain only English tweets, removing links and "stop words" (words such as the,a , by...) that would just hinder the process.

We then tokenized the tweets by words, that is splitting each word in the content while homogenizing the text

```{r}
#text analysis
data("stop_words")

Tweets_unnested <- IRAtweets %>%
  filter(language == "English",
         !account_category == "NonEnglish",
         !account_category == "Unknown")%>%
  mutate(Tweet_n = row_number(content))%>%
  mutate(content = replace_url(content, pattern = qdapRegex::grab("rm_url"), replacement = ""))%>%
  unnest_tokens(word, content) %>%
  anti_join(stop_words)%>%
  filter(!word == "rt") %>%
  mutate(word = gsub(x = word, pattern = "[0-9]+|[[:punct:]]|\\(.*\\)", replacement = "")) %>%
  filter(!word == "") 

Tweets_unnested %>%
  count(word, sort = T)

Tweets_unnested%>%
  group_by(account_category)%>%
  select(word, account_category)%>%
  count(word, sort = T)%>%
  pivot_wider(names_from = account_category, values_from = n)

tweet_words <- Tweets_unnested%>%
  count(account_category, word, sort = T)

tot_words <- tweet_words %>%
  group_by(account_category)%>%
  summarise(tot = sum(n))

tweet_words<- left_join(tweet_words, tot_words) %>%
  mutate(percent_freq = (n/tot)*100) %>%
  ungroup() %>%
  filter(!word == "")
head(tweet_words)
```

The most common words in the tweets are "news", "trump" and "police", by grouping the words by account we can see that those words are fairly common also by controlling for the different account types, further confirmed by the percentage frequencies of the same words.

Then we analyzed the word's tf-idf, the term frequency-inverse document frequency, which is a measure that assigns lower weight to words that are frequently used and a higher one to the uncommon ones.

The idea is to find a middle point between the two, as some words may be important to describe a text, while not being the most commonly used.

```{r}
tweet_tf_idf <- tweet_words %>% 
  bind_tf_idf(word, account_category,n)

# thus tf = term frequency, so idf / tf-idf = 0 for extremely common words, and increasing for the rarity of the terms

tweet_tf_idf %>%
  filter(!account_category == "HashtagGamer", #just returned random hashtags
         !account_category == "Commercial")%>% #just returned ads
  select(-tot)%>%
  arrange(desc(tf_idf))

tweet_tf_idf %>%
  filter(!account_category == "HashtagGamer", #just returned random hashtags
         !account_category == "Commercial")%>% #returned ads
  group_by(account_category)%>%
  slice_max(tf_idf, n = 10) %>%
  ungroup() %>%
  ggplot(aes(tf_idf, fct_reorder(word,tf_idf), fill = account_category))+
  geom_col(show.legend = F)+
  facet_wrap(~account_category, ncol = 2, scales = "free")+
  theme_bw()+
  labs(x = "tf-idf",
       y = NULL,
       title = "High tf-idf words by account groups")+
  paletteer::scale_fill_paletteer_d("nbapalettes::lakers_city")
  
```

The high tf-idf words, plot by account category, highlight common topics to the 2016 elections and other high impact words (some of which were previously hashtags, but were included to not forego any information) like "islam kills", "black skin is not a crime", "food poisoning", and "stay woke".

### Bi-gram Analysis

Tokenization isn't only possible at the singular worlds level, we can tokenize according to "n-grams", which are series of n words.

We decided to focus on Bigrams, series of 2 words, and repeat some of the previous text analysis.

```{r}
#bigram tokenization and analysis
#bigrams are series of 2 words (grey fox is fast = grey fox / fox is / is fast)
tweets_bigrams <- IRAtweets %>%
  filter(language == "English",
         !account_category == "NonEnglish",
         !account_category == "Unknown",)%>%
  mutate(Tweet_n = row_number(content))%>%
  mutate(content = replace_url(content))%>%
  unnest_tokens(bigram, content, token = "ngrams", n = 2) %>%
  filter(!is.na(bigram))

bigrams_sep <- tweets_bigrams %>%
  separate(bigram, c("word1", "word2"), sep = " ")

bigrams_filter <- bigrams_sep %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)

bigrams_count <- bigrams_filter %>% 
  count(word1, word2, sort = TRUE)

bigrams_count


bigrams_united <-  bigrams_filter %>%
  unite(bigram, word1, word2, sep = " ")

```

This gives us additional context on the most common words, like "world news" and "donald trump"

We can also repeat the tf_idf analysis with bigrams to capture additional context that might have been discarded by single-word analysis.

```{r}
bigram_tf_idf <- bigrams_united %>%
  count(account_category, bigram) %>%
  bind_tf_idf(bigram, account_category, n) %>%
  arrange(desc(tf_idf))
bigram_tf_idf


```

Again, bigrams like "poisoned turkey", "isis accounts", reveal patterns that weren't captured by the previous tf-idf plot

```{r}
bigram_tf_idf %>%
  filter(!account_category == "HashtagGamer", #just returned random hashtags
         !account_category == "Commercial")%>% #returned ads
  group_by(account_category)%>%
  slice_max(tf_idf, n = 10) %>%
  ungroup() %>%
  ggplot(aes(tf_idf, fct_reorder(bigram,tf_idf), fill = account_category))+
  geom_col(show.legend = F)+
  facet_wrap(~account_category, ncol = 2, scales = "free")+
  theme_bw()+
  labs(x = "tf-idf",
       y = NULL,
       title = "High tf-idf bigrams by account groups")+
  paletteer::scale_fill_paletteer_d("nbapalettes::bucks_city")
```

Additionally, the bigram combinations can be plot in a network graph:

```{r}
bigram_igraph <- bigrams_count %>%
  filter(n > 55) %>%
  graph_from_data_frame()

set.seed(808)
arrow <- grid::arrow(type = "closed", length = unit(.15, "inches"))

ggraph(bigram_igraph, layout = "fr") +
  geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
                 arrow = arrow, end_cap = circle(.07, 'inches'))+
  geom_node_point(color = "lightgreen", size = 5) +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
  labs(title = "Combinations of words in bigrams")+
  theme_void()

```

Arrows represent directional relationships between pairs of words, and distance is irrelevant.

### Sentiment Analysis

Sentiment analysis is a text analysis practice that assigns single words to sentiments as a method to interpret the emotion and feeling that a word is trying to convey.

This block filters out the word "trump" (to avoid misinterpretation in sentiment analysis) and joins the dataset with the NRC sentiment lexicon to classify words into sentiment categories. The result is a dataset of words with assigned sentiments.

```{r}
#sentiment analysis
#sentiment lexicons must be installed 

tweet_words %>%
  filter(!word == "trump")%>% #Trump (president) is interpreted as trump (surprise)
  inner_join(get_sentiments("nrc"), relationship = "many-to-many")

nrc_tweets <- Tweets_unnested %>%
  inner_join(get_sentiments("nrc"), relationship = "many-to-many")%>%
  filter(!sentiment == "positive",
         !sentiment == "negative",
         !word == "trump",)%>%
  mutate(method = "NRC")

nrc_posneg <-  Tweets_unnested %>%
  inner_join(get_sentiments("nrc"), relationship = "many-to-many")%>%
  filter(sentiment == "positive" | sentiment == "negative")%>%
  mutate(method = "NRC")

bing_tweets <- Tweets_unnested %>%
  inner_join(get_sentiments("bing"), relationship = "many-to-many")%>%
   filter(!word == "trump",
         !word == "breaking")%>%
  mutate(method = "Bing")

nrc_tweets %>%
  count(sentiment) %>%
  ggplot(aes(sentiment,n))+
  geom_bar(stat = "identity", fill = "#8785B2FF")+ scale_fill_paletteer_d("nbapalettes::bucks_city2")+
  theme_bw()+
  labs(x = "Sentiments",y = "tweets ", title = "Overall Sentiments in tweets")

AccCat_nrc <- nrc_tweets %>%
  group_by(account_category)%>%
  count(sentiment) %>%
  ggplot(aes(sentiment,n))+
  geom_bar(stat = "identity",aes(fill = account_category))+
  facet_wrap(~account_category, ncol = 2)+
  theme_bw()+
  labs(x = "Sentiments",y = "tweets ", title = "Sentiments in tweets by account category")+
  scale_fill_paletteer_d("rcartocolor::Temps")+
   theme(axis.text.x = element_text(angle = 45, hjust = 1))

nrc_tweets %>%
  group_by(account_category)%>%
  count(sentiment) %>%
  mutate(n = scale(n,center=min(n),scale=diff(range(n))))%>%
  ggplot(aes(sentiment,n))+
  geom_bar(stat = "identity",aes(fill = account_category))+
  facet_wrap(~account_category, ncol = 2)+
   theme_bw()+
  labs(x = "Sentiments",y = " share", title = "Sentiment share in tweets by account category")+
  scale_fill_paletteer_d("rcartocolor::Temps")+
   theme(axis.text.x = element_text(angle = 45, hjust = 1))

bing_tweets %>%
  count(account_category, score = Tweet_n %/% 100, sentiment) %>%
  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0)%>%
  mutate(sentiment = positive - negative) %>%
  ggplot(aes(score, sentiment, fill = account_category))+
  geom_col(show.legend = F)+
  facet_wrap(~account_category, ncol = 3, scales = "free_y")+
  theme_bw()+
  paletteer::scale_fill_paletteer_d("ggprism::colorblind_safe")+
  labs(x = "Tweets (hundreds)",y = "Score", title = "Positivity / Negativity by hundreds of tweets")
```

Now we compare the 3 lexicons.

```{r}
#Comparing the 3 lexicons
afinn_tweets <- Tweets_unnested %>%
   filter(!word == "trump",
         ! word == "breaking") %>%
  inner_join(get_sentiments("afinn"))%>%
  group_by(score = Tweet_n %/% 100) %>%
  summarise(sentiment = sum(value))%>%
  mutate(method = "AFINN")

bing_nrc_tweets <- bind_rows(bing_tweets ,nrc_posneg)%>%
  count(method, score = Tweet_n %/% 100, sentiment)%>%
  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0)%>%
  mutate(sentiment = positive-negative)

 bind_rows(afinn_tweets, bing_nrc_tweets) %>%
  ggplot(aes(score, sentiment, fill = method))+
  geom_col(show.legend = F)+
  facet_wrap(~method, ncol = 1, scales = "free_y" )+
  paletteer::scale_fill_paletteer_d("ggthemes::excel_Gallery")+
  theme_bw()+
   labs(x = "Tweets (thousands)",y = "Score", title = "Comparing different lexicons")

#NRC is notoriously known as a more positive lexicon than others 
 get_sentiments("nrc") %>% 
  filter(sentiment %in% c("positive", "negative")) %>% 
  count(sentiment)
 
 get_sentiments("bing") %>% 
  count(sentiment)
#NRC has a higher ratio of positive words
 
bing_wc <- bing_tweets %>%
  count(word, sentiment, sort = TRUE) %>%
  ungroup() %>%
  group_by(sentiment) %>%
  slice_max(n, n = 12) %>%
  ungroup %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n,word,fill = sentiment))+
  geom_col(show.legend = F)+
  facet_wrap(~sentiment, scales = "free_y")+
  labs(x = "Word count",
       y = NULL,
       title = "Word count by sentiment, Bing Lexicon")+
  theme_bw()+
  paletteer::scale_fill_paletteer_d("suffrager::classic")
  

nrc_wc <- nrc_posneg %>%
  count(word, sentiment, sort = TRUE) %>%
  ungroup() %>%
  group_by(sentiment) %>%
  slice_max(n, n = 12) %>%
  ungroup %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n,word,fill = sentiment))+
  geom_col(show.legend = F)+
  facet_wrap(~sentiment, scales = "free_y")+
  labs(x = "Word count",
       y = NULL,
       title = "Word count by sentiment, NRC Lexicon")+
  theme_bw()+
  paletteer::scale_fill_paletteer_d("nbapalettes::sixers_retro")
 
grid.arrange(bing_wc, nrc_wc)

tf_idf_sentiment <- tweet_words %>% 
  bind_tf_idf(word, account_category,n) %>%
  inner_join(get_sentiments("nrc"), relationship = "many-to-many")%>%
   filter(!sentiment == "positive",
         !sentiment == "negative",
         !word == "trump",)
  


tf_idf_sentiment %>%
  filter(!account_category == "HashtagGamer", #just returned random hashtags
         !account_category == "Commercial", #returned ads
         sentiment %in% c("anger", "disgust", "fear", "sadness", "trust"))%>% 
  group_by(account_category)%>%
  slice_max(tf_idf, n = 10) %>%
  ungroup() %>%
  ggplot(aes(tf_idf, fct_reorder(word,tf_idf), fill = sentiment))+
  geom_col(show.legend = F)+
  facet_wrap(~sentiment , ncol = 2, scales = "free")+
  theme_bw()+
  labs(x = "tf-idf",
       y = NULL,
       title = "High tf-idf words by sentiment")+
  scale_fill_paletteer_d("ButterflyColors::parides_zacynthus_polymetus")
```

We can observe that NRC is more positive than the other lexicons, even after correcting the issue with Trump being interpreted as a word of "surprise".

### LDA Modelling

We proceed to use LDA analysis to create a model that splits the tweets we found into topic. We take each tweet as a document and its text as a corpus. The tweets are then aggregated through LDA into topics, which returns us a model for how likely each word is to come out each topic.

We proceed to use LDA analysis to create a model that splits the tweets we found into topic. We take each tweet as a document and its text as a corpus. The tweets are then aggregated through LDA into topics, which returns us a model for how likely each word is to come out each topic.

First we have to train the model, so we're going to divide the tweets into training and testing data with an 80/20 split.

```{r}
# Data-term matrix

set.seed(505)
Tweets_Partition <- createDataPartition(Tweets_unnested$Tweet_n,p=0.8,list=FALSE)
Tweets_Train <- Tweets_unnested[Tweets_Partition,] #train data, 80%
Tweets_Test <-  Tweets_unnested[-Tweets_Partition,] #Test data, 20%

dtm_train <- Tweets_Train %>%
  count(Tweet_n,word) %>%
  cast_dtm(Tweet_n, word, n)

dtm_test <- Tweets_Test %>%
  count(Tweet_n,word) %>%
  cast_dtm(Tweet_n, word, n)
```

We then run the model for different numbers of topic, and evaluate its accuracy (which is measured by the model's perplexity). We toggle this codeblock's execution off because it takes a long time to run, but the model is most accurate with 25 topics (as shown by the plot).

It's worth noting that perplexity remains high, despite the singular iteration, but that's to be expected: even in human interpretation it's recognised that Tweets, by their very nature, are often ambiguous. Astroturf-related tweets, which by their own objective try to be even more ambiguous, are bound to share that quality.

```{r}
#| execute: false

#find the correct number of topics 
 # updog <- FindTopicsNumber(
 #   dtm_train,
 #   topics = seq(from = 2, to = 60, by = 1),
 #   metrics = c("Griffiths2004", "CaoJuan2009", "Arun2010"),
 #   method = "Gibbs",
 #   mc.cores = 4L,
 #   verbose = TRUE)
 # 
 # FindTopicsNumber_plot(updog)
# 25 is chosen as the optimal number of topics
```

![](images/LDA_topic_minimization.jfif)

That being said, we can clearly observe patterns of topic forming within the model

```{r}
#LDA model (may take a few minutes)
trainLDA <- LDA(dtm_train, k = 25, method = "Gibbs") #"k = " for topics
trainLDAtidy_terms <- tidy(trainLDA, matrix = "beta")

terms_train <- trainLDAtidy_terms %>%
  group_by(topic)%>%
  slice_max(beta, n = 10) %>%
  ungroup() %>%
  arrange(topic, beta)

terms_train %>%
  filter(topic %in% (1:9))%>% #visualize the topics with the most word %
  mutate(term = reorder_within(term, beta, topic))%>%
  ggplot(aes(beta, term, fill = factor(topic)))+
  geom_col(show.legend = F)+
  facet_wrap(~topic, scales = "free_y")+
  scale_y_reordered()+
   theme_bw()+
  labs(x = "Probability",
       y = "Word",
       title = "Highest probability words by topic")

#per-topic word probabilities
termstrain_wide <- terms_train %>% 
  mutate(topic = paste0("topic", topic)) %>%
  pivot_wider(names_from = topic, values_from = beta)
termstrain_wide #probability of the top 10 words appearing in topic

#per-document word probabilities
trainLDAtidy_docs <- tidy(trainLDA, matrix = "gamma")
trainLDAtidy_docs #share of words from that document generated from that topic

#model evaluation
perplexity(trainLDA, dtm_train, estimate_theta = F)
perplexity(trainLDA, dtm_test, estimate_theta = T )
```

We can observe several clear topic being distinguished, some of which along political lines, like the Obama presidency, Hillary Clinton and Donald Trump. Some of which along cultural topics, like workouts or sports.

We then evaluate sentiment by topic

```{r}
#sentiment by topic
nrc_LDAtrain <- trainLDAtidy_terms %>%
  rename(word = term) %>%
  inner_join(get_sentiments("nrc"), relationship = "many-to-many")%>%
  filter(!sentiment == "positive",
         !sentiment == "negative",
         !word == "trump",)%>%
  mutate(method = "NRC")

nrc_LDAtrain %>%
  filter(topic %in% (1:9))%>%
  group_by(topic)%>%
  slice_max(beta, n = 50) %>%
  count(sentiment) %>%
  ggplot(aes(sentiment,n))+
  geom_bar(stat = "identity",aes(fill = topic), show.legend = F)+
  facet_wrap(~topic, ncol = 3)+
   theme_bw()+
  labs(x = "Sentiments",y = "count", title = "Sentiment count of the 50 most probable words by topic")+
  theme(axis.text.x = element_text(angle = 45, hjust = 1))+
  paletteer::scale_fill_paletteer_c("ggthemes::Sunset-Sunrise Diverging")
```

We can observe that topic Trump is correlated with high feelings of trust, hope. This is in stark contrast with the feelings around other political candidates. This is far and away the strongest visible pattern of this analysis, showing clearly the intent behind the operation: through subtle and hard-to-detect political messaging, to associate Trump with hope for the future and positivity, and his opposition with negativity.

## Conclusion

### Future Research
